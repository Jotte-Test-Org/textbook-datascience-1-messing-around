{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7664a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2c68fc6",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "## Boosting\n",
    "\n",
    "Boosting focuses on sequentially improving the model's weaknesses.\n",
    "\n",
    "It gives more weight to misclassified instances, allowing the model to learn from its mistakes.\n",
    "\n",
    "\n",
    "### Boosting with Decision Trees - AdaBoost\n",
    "\n",
    "AdaBoost (Adaptive Boosting) is a popular boosting algorithm with decision trees.\n",
    "\n",
    "Weak learners (usually shallow trees) are trained sequentially.\n",
    "\n",
    "Each tree corrects the mistakes of the previous ones.\n",
    "\n",
    "Initially, all data points have equal weights.\n",
    "\n",
    "Misclassified points receive higher weights in subsequent iterations.\n",
    "\n",
    "Final prediction is a weighted sum of individual tree predictions.\n",
    "\n",
    "\n",
    "#### Tuning Parameters (ie hyperparameters)\n",
    "\n",
    "The primary ones include:\n",
    "- **base_estimator:** The base model used for boosting (e.g., DecisionTreeClassifier).\n",
    "- **n_estimators:** Number of weak learners (base models) to train sequentially. (50)\n",
    "- **learning_rate:** The contribution of each weak learner to the final prediction. (1)\n",
    "\n",
    "\n",
    "### Benefits of Boosting\n",
    "\n",
    "Why use boosting?\n",
    "- Improved accuracy: Boosting focuses on difficult-to-learn instances, improving overall model performance.\n",
    "- Versatility: Works well with various base learners.\n",
    "\n",
    "## Bagging vs Boosting\n",
    "\n",
    "#### Remember our bias-variance tradeoff??\n",
    "\n",
    "Bagging aims to reduce variance (overfitting)\n",
    "\n",
    "Boosting focuses on reducing bias (underfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e2043d",
   "metadata": {},
   "source": [
    "## Boosting \n",
    "\n",
    "|   |   |\n",
    "|:--|:--|\n",
    "| <img src=\"http://github.com/david-biron/DATA221imgs/blob/main/icon_definition.png?raw=true\" width=\"50\" height=\"\"> | **Boosting** combines weak learners (decision trees) **sequentially**. <br> The output of a weak learner are assigned **weights**. <br> The incorrect classifications of the current weak learner  <br> are given higher weights such that they have more <br> representations/influence as inputs to the next weak learner. |\n",
    "\n",
    "\n",
    "#### Note\n",
    "\n",
    "Boosting can be a powerful alternative to bagging. Instead of 'egalitarian' aggregation of predictions, boosters tweak weak learners by sequentially preferring to correct previous errors. \n",
    "\n",
    "In Gradient Boosting, an individual model is trained on the **residuals** of its predecessor, such that it learns from previous errors.\n",
    "\n",
    "### An outline of the AdaBoost algorithm\n",
    "\n",
    "* Each record in the training set is assigned a weight. These weight change every step <br/> and influence the representation of the corresponding records in the next step. <br/> Initially, all weights are equal so the training subset is randomly selected. \n",
    "\n",
    "<br/> \n",
    "\n",
    "* Select a training subset according to the weights.\n",
    "* Train the current stump using the current training subset. \n",
    "* Assign higher weights to wrongly classified records. <br/> They will be more likely represented in the input for the next iteration.\n",
    "* Assign an overall weight to the stump based on its accuracy (for instance). \n",
    "* Iterate until the training data fits without errors or the specified maximum number of stumps was reached.\n",
    "\n",
    "<br/>\n",
    "\n",
    "* Weighted-vote across all of the stumps using the overall weights assigned to each of them.\n",
    "\n",
    "**Note**: \n",
    "\n",
    "* The more succesful the stump, the more the sample weights of its errors are scaled up (emphasized). <br/> They are scaled down (de-emphasized) when the previous stump had a negative vote weight. \n",
    "* The more succesful the stump, the more the sample weights for correct classifications are scaled down (de-emphasized). <br/> They are scaled up (emphasized) when the previous stump had a negative vote weight.  \n",
    "\n",
    "### Finally, after generating a forest of stumps: \n",
    "\n",
    "To classify a given record:\n",
    "* Sum the $Stump\\_Vote\\_Weight$s for the stumps that give $0$.\n",
    "* Sum the $Stump\\_Vote\\_Weight$s for the stumps that give $1$. \n",
    "* The larger of the two sums determines the classification decision. \n",
    "\n",
    "#### The base estimator can be changed \n",
    "\n",
    "For instance, instead of the (default) decision stumps AdaBoost can use [Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html).\n",
    "\n",
    "This may or may not improve things, depending on the dataset... \n",
    "\n",
    "\n",
    "#### Pros \n",
    "\n",
    "AdaBoost is not (particularly) prone to overfitting. \n",
    "\n",
    "#### Cons \n",
    "\n",
    "AdaBoost is sensitive to noise/outliers and can be slow. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
