{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973739bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60ed9367",
   "metadata": {},
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7665f",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "## Bagging (Bootstrap Aggregation)\n",
    "\n",
    "Train multiple instances of the same model on different subsets of the training data\n",
    "\n",
    "Each subset is created by sampling *with* replacement - bootstrapping!!\n",
    "\n",
    "The final prediction is an average (for regression) or a majority vote (for classification) of the individual models\n",
    "\n",
    "\n",
    "### Bagging with Decision Trees - Random Forests\n",
    "\n",
    "Random Forest builds multiple decision trees and combines their predictions.\n",
    "\n",
    "Each tree is trained on a different subset of data (bootstrap samples).\n",
    "\n",
    "Random subsets of features are considered for splitting at each node.\n",
    "\n",
    "The final prediction is an average (regression) or majority vote (classification) of individual tree predictions.\n",
    "\n",
    "#### Tuning Parameters (ie hyperparameters)\n",
    "\n",
    "The main ones include:\n",
    "- **n_estimators:** Number of trees in the forest. (100)\n",
    "- **max_depth:** Maximum depth of each tree. (None)\n",
    "- **min_samples_split:** Minimum number of samples required to split an internal node. (2)\n",
    "- **min_samples_leaf:** Minimum number of samples required to be at a leaf node. (1)\n",
    "- **max_features:** Number of features to consider when looking for the best split. (sqrt(n_features))\n",
    "\n",
    "\n",
    "### Grid Search\n",
    "\n",
    "Selecting the right hyperparameters is crucial but manually tuning hyperparameters can be challenging.\n",
    "\n",
    "**Grid search** is a systematic method for hyperparameter tuning. It involves evaluating a predefined set of hyperparameter combinations to find the best-performing configuration.\n",
    "\n",
    "Imagine a grid where each axis represents a hyperparameter, and the points on the grid are combinations of hyperparameter values. Grid search exhaustively explores this grid, testing each combination.\n",
    "\n",
    "\n",
    "Grid search involves three main components:\n",
    "\n",
    "- **Hyperparameter Space:** The range or values to be explored for each hyperparameter.\n",
    "- **Scoring Metric:** The performance metric used to evaluate each combination.\n",
    "- **Cross-Validation:** The technique used to assess performance robustly.\n",
    "\n",
    "For effective grid search:\n",
    "- Start with a broad search space.\n",
    "- Refine based on initial results.\n",
    "\n",
    "### Benefits of Bagging\n",
    "\n",
    "Why use bagging?\n",
    "- Reduces overfitting: By training on different subsets, models are less likely to memorize the training data.\n",
    "- Increased stability: Ensemble models are less sensitive to noise and outliers in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2bc3d1",
   "metadata": {},
   "source": [
    "## Bagging - a Random Forest\n",
    "\n",
    "### A decision tree is prone to overfitting, so why settle for a single tree? \n",
    "\n",
    "|   |   |\n",
    "|:--|:--|\n",
    "| <img src=\"https://github.com/david-biron/DATA221imgs/blob/main/icon_definition.png?raw=true\" width=\"50\" height=\"\"> | **Ensemble methods** aggregate the result from a set of classifiers (or regression <br> models).  Individual predictions of 'weak learners' are aggregated by the <br> majority rule (or averaging) to identify the most popular result. |\n",
    "\n",
    "|   |   |\n",
    "|:--|:--|\n",
    "| <img src=\"https://github.com/david-biron/DATA221imgs/blob/main/icon_definition.png?raw=true\" width=\"50\" height=\"\"> | **Bagging (or bootstrap aggregation)** is an ensemble learning method: <br> a random sample of data in a training set is selected **with replacement**. Several <br> data samples are generated, used to train models independently, and the  result <br> is aggregated by a majority  rule (classification) or averaging (regression). <br> Typically, bagging reduces variance. |\n",
    "\n",
    "|   |   |\n",
    "|:--|:--|\n",
    "| <img src=\"https://github.com/david-biron/DATA221imgs/blob/main/icon_definition.png?raw=true\" width=\"50\" height=\"\"> | **Feature randomness (or 'feature bagging')** generates random subsets of features. <br>  This reduces the correlations between the resulting classifiers (or regression models). |\n",
    "\n",
    "|   |   |\n",
    "|:--|:--|\n",
    "| <img src=\"https://github.com/david-biron/DATA221imgs/blob/main/icon_definition.png?raw=true\" width=\"50\" height=\"\"> | A **random forest** combines bagging and feature randomness to create multiple low <br> correlated decision trees. Each individual tree is a weak learner. The forest aggregates <br> their results (majority rule or average) to identify the most popular result. <br> Typically, this reduces the risk of overfitting and increases the accuracy of predictions.  |\n",
    "\n",
    "\n",
    "\n",
    "### Hyperparameters of a random forest and how to search for them (`RandomizedSearchCV`)\n",
    "\n",
    "* The **number of trees** (`n_estimators`): more trees may well improve performance but will require more time/memory for training. \n",
    "\n",
    "* The **maximum depth** of each decision tree (`max_depth`): excessively high (/low) values $\\rightarrow$ overfitting (/underfitting).\n",
    "\n",
    "* The **number of features** to consider when looking for the best split (`max_features`). \n",
    "\n",
    "[There are others...](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "\n",
    "### Pros and cons of random forests\n",
    "\n",
    "**Pros** \n",
    "* Reduced risk of overfitting ('reduce variance'). \n",
    "* Can be used for classification or regression.\n",
    "* Feature importance is simple to calculate.  \n",
    "\n",
    "**Cons** \n",
    "* Time/memory consuming (the more trees...)\n",
    "* Interpretability is lost as compared, e.g., to a single tree. \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
